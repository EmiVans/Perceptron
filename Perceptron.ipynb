{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo de perceptron con funcion de activacion sigmoidal.**\n"
      ],
      "metadata": {
        "id": "4Ebjw7cHhxRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicion de las clases que ocuparemos."
      ],
      "metadata": {
        "id": "gd4fu1pp-JJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "class Node:\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.value)\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "# Empezamos a crear nuestras sub clases que tambien seran nodos\n",
        "# Primero definimos el que sera nuestro nodo principal para tres de nuestros modelo\n",
        "# el cual es el nodo de pre-activacion\n",
        "class PreActivation(Node):\n",
        "    def __init__(self, input_size):\n",
        "        self.weight = np.random.randn(input_size)\n",
        "        self.bias = np.random.randn(1) # el bias es un tensor\n",
        "        self.inputs = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.inputs = x\n",
        "        self.out = np.dot(x, self.weight) + self.bias\n",
        "        return self.out\n",
        "\n",
        "\n",
        "    # Grad_out es el gradiente calculado en el nodo sigmoide\n",
        "    def backward(self, grad_out):\n",
        "        self.grad_weight = grad_out * self.inputs\n",
        "        self.grad_bias = np.sum(grad_out, axis=0)\n",
        "        return self.grad_weight, self.grad_bias\n",
        "\n",
        "    # Funcion escalonada del perceptron\n",
        "    def predict(self,x):\n",
        "        return np.where( self.forward(x)>0, 1, 0)\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "# Definimos un nodo para la funcion de activacion Sigmoide\n",
        "class Sigmoide(Node):\n",
        "    def forward(self, z):\n",
        "        self.out = 1 / (1 + np.exp(-z))\n",
        "        return self.out\n",
        "\n",
        "    # Grad_out es el gradiente calculado en el nodo CrossEntropy\n",
        "    def backward(self, grad_out):\n",
        "        grad_in = self.out * (1 - self.out)\n",
        "        return grad_in * grad_out\n",
        "\n",
        "    def predict(self,z):\n",
        "        return np.where( self.forward(z)>0.5, 1, 0)\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "# Definimos un nodo para la funcion de entropia cruzada\n",
        "# La cual es necesaria para hacer el calculo del gradiente\n",
        "class CrossEntropy(Node):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        epsilon = 1e-15 # para evitar caer en un log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
        "        self.out = -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "        return self.out\n",
        "    def backward(self, y_pred, y_true):\n",
        "        if y_true == 1:\n",
        "            return -(1 / y_pred)\n",
        "        else:\n",
        "            if y_true == 0:\n",
        "                return 1 / (1-y_pred)\n",
        "        return self.out"
      ],
      "metadata": {
        "id": "xhFJUj3V41iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo a partir de datos generados con la funcion \"make_classification\" de scikit-learn."
      ],
      "metadata": {
        "id": "WCaVFX5W-tpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2, random_state=10)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "r_Thyy-j5Oyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializamos nuestros nodos\n",
        "np.random.seed(42)\n",
        "# 1.- Nodo de pre-activacion\n",
        "pre_activacion = PreActivation(2)\n",
        "# 2.- Node de activacion (sigmoide)\n",
        "sigmoide = Sigmoide()\n",
        "# 3.- Nodo de entropia cruzada\n",
        "entropia_cruzada = CrossEntropy()\n",
        "\n",
        "# inicializamos nuestra variable para tasa de aprendizaje y epocas\n",
        "lr = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# paso forward para 1 iteracion\n",
        "history = []\n",
        "for epoch in range(epochs):\n",
        "    history_for_epoch = []\n",
        "    for i in range(len(y_train)):\n",
        "        # paso forward\n",
        "        z = pre_activacion.forward(x_train[i])\n",
        "        y_pred = sigmoide.forward(z)\n",
        "        loss = entropia_cruzada.forward(y_pred, y_train[i])\n",
        "\n",
        "        # paso backward\n",
        "        grad_loss = entropia_cruzada.backward(y_pred, y_train[i])\n",
        "        grad_sigmoid = sigmoide.backward(grad_loss)\n",
        "        grad_weights, grad_bias = pre_activacion.backward(grad_sigmoid)\n",
        "\n",
        "        # actualizacion de los pesos\n",
        "        pre_activacion.weight -= lr * grad_weights\n",
        "        pre_activacion.bias -= lr * grad_bias\n",
        "        history_for_epoch.append(loss)\n",
        "    history.append(np.mean(history_for_epoch))\n",
        "    print(f'Epoch: {epoch + 1}, loss: {loss}')"
      ],
      "metadata": {
        "id": "hQIl8xYD5PnV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados del entrenamiento"
      ],
      "metadata": {
        "id": "kvq91CDx_j_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "z = pre_activacion.forward(x_test)\n",
        "y_pred_test = sigmoide.predict(z)\n",
        "report = classification_report(y_test, y_pred_test)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "NckEkRYo6AYQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grafica que modela el comportamiento de la perdida promedio a lo largo de las epocas."
      ],
      "metadata": {
        "id": "Yu7qfMQU_qOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(range(epochs)), history)\n",
        "plt.title('Comportamiento de la Pérdida promedio del modelo a lo largo de las épocas')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r7urJYkTq9ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_test[:,0], x_test[:,1], c=y_pred_test)\n",
        "plt.title('Datos clasificados')\n",
        "plt.xlabel('Caracteristiica 1')\n",
        "plt.ylabel('Caracteristica 2')\n",
        "plt.show(\"hpoa\")\n"
      ],
      "metadata": {
        "id": "b_1XDYmlN2tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparacion de la funcion escalonada con la regresion logistica."
      ],
      "metadata": {
        "id": "hN19X6PU_58S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_test)\n",
        "y_escalonada = pre_activacion.predict(x_test)\n",
        "print(y_escalonada)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q_6I8CQe6w2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo de perceptron con una funcion de regresion lineal como funcion de activacion:**\n"
      ],
      "metadata": {
        "id": "ko22Qw88uNmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicion de las clases que ocuparemos."
      ],
      "metadata": {
        "id": "L2q2KFT-AH9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------------------------------------------\n",
        "# Empezamos a crear nuestras sub clases que tambien seran nodos\n",
        "# Primero definimos el que sera nuestro nodo principal para tres de nuestros modelo\n",
        "# el cual es el nodo de pre-activacion\n",
        "class PreActivation(Node):\n",
        "    def __init__(self, input_size):\n",
        "        self.weight = np.random.randn(input_size) # el peso o los pesos son un tensor\n",
        "        self.bias = np.random.randn(1) # el bias es un tensor\n",
        "        self.inputs = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.inputs = x\n",
        "        self.out = np.dot(x, self.weight) + self.bias\n",
        "        return self.out\n",
        "\n",
        "    # aqui el grad_out es el gradiente calculado en el nodo sigmoide\n",
        "    def backward(self, grad_out):\n",
        "        #self.grad_weight = np.dot(self.inputs.T, grad_out)\n",
        "        self.grad_weight = grad_out * self.weight\n",
        "        self.grad_bias = np.sum(grad_out, axis=0)\n",
        "        return self.grad_weight, self.grad_bias\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "# Definimos un nodo para la funcion de entropia cruzada\n",
        "# La cual es necesaria para hacer el calculo del gradiente\n",
        "class mean_square_error(Node):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        #return 2*(y_true-y_pred)\n",
        "        return (y_pred-y_true)\n",
        "\n",
        "class MeanSquareError(Node):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.out = np.mean((y_pred - y_true.reshape(-1,1)) ** 2)\n",
        "        #self.out = mean_squared_error(y_true, y_pred)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        self.grad = (2 * (y_pred - y_true.reshape(-1,1))) / y_true.size\n",
        "        return self.grad"
      ],
      "metadata": {
        "id": "-zBkpwk7umki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo."
      ],
      "metadata": {
        "id": "Eu4n_ouCAYjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializamos nuestros nodos\n",
        "np.random.seed(42)\n",
        "# 1.- Nodo de pre-activacion\n",
        "pre_activacion = PreActivation(input_size = x_train.shape[1])\n",
        "# 2.- Node de mean square error\n",
        "objetive_funtion = mean_square_error()\n",
        "\n",
        "# inicializamos nuestra variable para tasa de aprendizaje y epocas\n",
        "lr = 0.01\n",
        "epochs = 200\n",
        "\n",
        "# paso forward para 1 iteracion\n",
        "history = []\n",
        "for epoch in range(epochs):\n",
        "    history_for_epoch = []\n",
        "    # paso forward\n",
        "    y_pred = pre_activacion.forward(x_train)\n",
        "    loss = objetive_funtion.forward(y_pred, y_train)\n",
        "\n",
        "    # paso backward\n",
        "    grad_loss = np.mean(objetive_funtion.backward(y_pred, y_train))\n",
        "    grad_weights, grad_bias = pre_activacion.backward(grad_loss)\n",
        "\n",
        "    # actualizacion de los pesos\n",
        "    pre_activacion.weight -= lr * grad_weights\n",
        "    pre_activacion.bias -= lr * grad_bias\n",
        "    history.append(loss)\n",
        "    print(f'Epoch: {epoch + 1}, loss: {loss}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nVIrtlQ4316k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(range(epochs)), history)\n",
        "plt.title('Comportamiento de la Pérdida del Modelo a lo Largo de las Épocas')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "igTgVxXGBtE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluacion del modelo."
      ],
      "metadata": {
        "id": "F-Ic-xYZAfTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_test = pre_activacion.forward(x_train)\n",
        "MSE = mean_squared_error(y_train, y_pred_test)\n",
        "r2 = r2_score(y_train, y_pred_test)\n",
        "print(\"El MSE es: \", MSE)\n",
        "print(\"El R2 es: \", r2)"
      ],
      "metadata": {
        "id": "e-qFelLD1Sv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}